{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Langchain Application as a Custom MLServer Model\n",
    "\n",
    "This notebook runs through the deployment of a simple LangChain application using a custom MLServer on a kind Kubernetes cluster. We are going to deploy the following [example/tutorial](https://python.langchain.com/docs/tutorials/rag/) from the langchain documentation. You will need an OpenAI API key, although note that it's easy to adjust the process for other LLM API providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start we'll need to create a virtual environment with all the required dependencies. We'll use conda:\n",
    "\n",
    "```sh\n",
    "conda create -n langchain-model python=3.10\n",
    "conda activate langchain-model\n",
    "```\n",
    "\n",
    "We'll also need to install all the dependencies\n",
    "\n",
    "```sh\n",
    "pip install mlserver\n",
    "pip install langchain-openai\n",
    "pip install beautifulsoup4\n",
    "pip install langchain\n",
    "pip install langchain-chroma\n",
    "pip install langchain-community\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the Custom MLServer Model that implements the LangChain application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlserver import types\n",
    "from mlserver.model import MLModel\n",
    "from mlserver.codecs import NumpyCodec, StringCodec\n",
    "from mlserver.types import InferenceRequest, InferenceResponse\n",
    "from mlserver.logging import logger\n",
    "\n",
    "import os\n",
    "import bs4\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "class LangChainApp(MLModel):\n",
    "    async def load(self) -> bool:\n",
    "        openai_api_key = self._settings.parameters.extra['openai_api_key']\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "        )\n",
    "        self.loader = WebBaseLoader(\n",
    "            web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "            bs_kwargs=dict(\n",
    "                parse_only=bs4.SoupStrainer(\n",
    "                    class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        docs = self.loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        self.rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        self.ready = True\n",
    "        logger.info(f\"---- loaded LangChain Application ----\")\n",
    "        return self.ready\n",
    "\n",
    "    def unpack_input(self, payload: InferenceRequest):\n",
    "        request_data = {}\n",
    "        for inp in payload.inputs:\n",
    "            if inp.name == 'query':\n",
    "                request_data[inp.name] = (\n",
    "                    NumpyCodec\n",
    "                    .decode_input(inp)\n",
    "                    .flatten()\n",
    "                    .tolist()\n",
    "                )\n",
    "        return request_data\n",
    "    \n",
    "    async def predict(self, payload: types.InferenceRequest) -> types.InferenceResponse:\n",
    "        unpacked_payload = self.unpack_input(payload)\n",
    "        response = self.rag_chain.invoke(unpacked_payload['query'][0])\n",
    "        outputs = [StringCodec.encode_output(\"response\", [response])]\n",
    "        return InferenceResponse(\n",
    "            id=payload.id,\n",
    "            model_name=self.name,\n",
    "            model_version=self.version,\n",
    "            outputs=outputs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above `load` method is responsible for initialising and setting up the `LangChain` Application. Note that in production you'd probably want to provision a vector database separately and insert the embedded chunks in a separate process to load. Because this is just a demo and we're only loading and processing a single webpage it is fine to do it all in the load method. But not if we deploy multiple replicas of this model it'll rerun the load method which you probably don't want if the vector database setup takes a long time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the above we need to write a `model-settings.json` file. This just contains the model configuration. In this case, we're going to pass the `openai_api_key` to the model here. But you could also include other settings if you wish. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"name\": \"langchain-model\",\n",
    "    \"implementation\": \"model.LangChainApp\",\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"openai_api_key\": <openai_api_key>\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these files, the `model.py` and the `model-settings.json` file should be stored in a folder, I've called mine `langchain_model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can test run all of the above using `mlserver start langchain_model/`. If everything is set up correctly you should now be able to query the model locally using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often employs techniques like Chain of Thought (CoT) to enhance performance by prompting the model to think step by step. Additionally, it can involve human input or task-specific instructions to guide the decomposition process.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"query\", \n",
    "            \"shape\": [1, 1], \n",
    "            \"datatype\": \"BYTES\", \n",
    "            \"data\": ['What is Task Decomposition?']\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8080/v2/models/langchain-model/infer\",\n",
    "    json=inference_request,\n",
    ")\n",
    "\n",
    "print(response.json()['outputs'][0]['data'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying with kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to try and deploy using Kubernetes. The base MLServer image doesn't come with the langchain dependency included so we'll need to include it ourselves. There are a couple of ways of doing this.\n",
    "\n",
    "1. We create a tarball file and bundle it up with the `model.py` and `model-settings.json` files. When we deploy these the load method will also unpack the dependencies so that the custom mlserver model has access to what it needs.\n",
    "2. We create a new docker image that includes the relevant dependencies and deploy it to a new server. If a model requires specific dependencies we can add a requirements field to the model.yaml and Seldon core will deploy the model on the correct server.\n",
    "\n",
    "We're going to take approach 1 here which is also talked about [here](https://mlserver.readthedocs.io/en/latest/examples/conda/README.html#serialise-our-custom-environment). Approach 2, in case your interested, is detailed [here](https://mlserver.readthedocs.io/en/latest/examples/custom/README.html#deployment).\n",
    "\n",
    "First, we need to use `conda-pack` to create the tar-ball. Run `conda install conda-pack` to download [conda-pack](https://conda.github.io/conda-pack/#conda-pack) which we'll use to create the serialized conda environment. We run:\n",
    "\n",
    "```\n",
    "conda pack --force -n langchain-model -o langchain_model/langchain-model.tar.gz\n",
    "```\n",
    "\n",
    "Now the `langchain_model` folder should contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mlangchain_model\u001b[0m\n",
      "├── \u001b[01;31mlangchain-model.tar.gz\u001b[0m\n",
      "├── model.py\n",
      "└── model-settings.json\n",
      "\n",
      "0 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "!tree langchain_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to update the `model-settings.json` file to reference the `.tar.gz` file.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"langchain-model\",\n",
    "    \"implementation\": \"model.LangChainApp\",\n",
    "    \"parameters\": {\n",
    "        \"environment_tarball\": \"./langchain-model.tar.gz\",\n",
    "        \"extra\": {\n",
    "            \"openai_api_key\": <openai_api_key>\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seldon-core deployment workflow requires that the model assets be stored on some external storage. Options include [minio](https://docs.seldon.io/projects/seldon-core/en/v2/contents/kubernetes/storage-secrets/index.html) but for simplicity, we'll use use google cloud storage. In particular, I've uploaded the model files to a Google bucket: `gs://llm-demos/llm-demos/langchain-example/langchain_model`. Note that I've removed the OpenAI api key from the `model-settings.json` file uploaded here. You'll want to set up your own key and google bucket, and replace the `storageUri` in the model yaml bellow to point at your bucket.\n",
    "\n",
    "If we now deploy the following `model.yaml` file onto a cluster running seldon-core-v2 we should successfully deploy the LangChain model:\n",
    "\n",
    "```yaml\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Model\n",
    "metadata:\n",
    "  name: langchain-model\n",
    "spec:\n",
    "  storageUri: \"gs://llm-demos/langchain-example/langchain_model\"\n",
    "  requirements:\n",
    "  - mlserver\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.mlops.seldon.io/langchain-model created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f model.yaml -n seldon-mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "def get_mesh_ip():\n",
    "    cmd = f\"kubectl get svc seldon-mesh -n seldon-mesh -o jsonpath='{{.status.loadBalancer.ingress[0].ip}}'\"\n",
    "    return subprocess.check_output(cmd, shell=True).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. This can be achieved through techniques like Chain of Thought (CoT) and Tree of Thoughts, which guide the model to think step by step and explore multiple reasoning paths. By doing so, it enhances the model's performance on complex tasks and provides insight into its reasoning process.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"query\", \n",
    "            \"shape\": [1, 1], \n",
    "            \"datatype\": \"BYTES\", \n",
    "            \"data\": ['What is Task Decomposition?']\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"http://{get_mesh_ip()}/v2/models/langchain-model/infer\",\n",
    "    json=inference_request,\n",
    ")\n",
    "\n",
    "data = response.json()['outputs'][0]['data'][0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Note\n",
    "\n",
    "Although it is possible to do the above when deploying LangChain applications with MLServer this isn't how we recommend Large language model applications be deployed in general. The problem with the above approach is that each component in the application, the vector database, and the embedding model are all packaged into a single model run on a single server. This means that:\n",
    "\n",
    "1. Single point of failure in the system\n",
    "2. You can't scale each component independently\n",
    "3. Data passed between components isn't exposed so debugging or auditing the data becomes harder\n",
    "\n",
    "Seldons suggested approach is to deploy large language models using our [LLM-module](https://www.seldon.io/solutions/llm-module). The LLM module provides a set of MLServer runtimes for each of the components that you might need in an LLM application. It allows you to deploy each component separately and then wire them up using Seldon core v2 [pipelines](https://docs.seldon.io/projects/seldon-core/en/v2/contents/pipelines/index.html). This way you get:\n",
    "\n",
    "- A truly distributed system\n",
    "- Components are reusable\n",
    "- The data coming in and out of each component can be examined\n",
    "- You can deploy things like drift detectors or explainers on your pipeline or for specific components\n",
    "- You can scale/autoscale components independently of the others\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
